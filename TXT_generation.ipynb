{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IliyaFrahani/SimpleTextGeneration/blob/main/TXT_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3R2ZeQeolXP"
      },
      "source": [
        "# ***text generation using keras and tensorflow*** \n",
        "* data set :\n",
        "    https://drive.google.com/file/d/1nhThs5ya5KQ77Nxyghm_Aoi4fo06zWYI/view?usp=share_link\n",
        "    \n",
        "    https://drive.google.com/file/d/1dtmFb8dxjVRS_-pI6dse8nngu0JmwGBO/view?usp=share_link\n",
        "\n",
        "    https://drive.google.com/file/d/1Ca4gXHAfCRzhTnm2f0FSM6OwVq5N5kmc/view?usp=share_link\n",
        "\n",
        "    https://drive.google.com/file/d/17JQzRfbh7Odh16IjZdRhl3DlDpC1ISCy/view?usp=share_link\n",
        "\n",
        "# *Write by iliya farahani*\n",
        "\n",
        "* data sets lang :\n",
        "  \n",
        "  persian\n",
        "\n",
        "  english"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2pv4dtP0Jcp"
      },
      "source": [
        "# ***Importing Library's :***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg3jJqiY0c2I"
      },
      "outputs": [],
      "source": [
        "# import lib\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "import json\n",
        "import re\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense , Dropout\n",
        "from keras.callbacks import TensorBoard, EarlyStopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3kOhYcR1wYe"
      },
      "source": [
        "# ***load random sentences json file dataset :***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySQ9wy682R4T"
      },
      "outputs": [],
      "source": [
        "# load json file\n",
        "data = open(\"/content/first-rows.json\")\n",
        "data = json.load(data)\n",
        "data = (data[\"rows\"])\n",
        "\n",
        "# func for cleaning data\n",
        "def clean(text):\n",
        "    text = re.sub('[0-9]+.\\t','',str(text))\n",
        "    text = re.sub('\\n ','',str(text))\n",
        "    text = re.sub('\\n',' ',str(text))\n",
        "    text = re.sub(\"'s\",'',str(text))\n",
        "    text = re.sub(\"-\",' ',str(text))\n",
        "    text = re.sub(\"â€” \",'',str(text))\n",
        "    text = re.sub('\\\"','',str(text))\n",
        "    text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
        "    text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
        "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
        "    return text\n",
        "\n",
        "# extrac just anserws from questions and anserws\n",
        "just_anserws = \"\"\n",
        "for i in data : \n",
        "  for j in (re.split(\"Human:\", i[\"row\"][\"texts\"]))[1::] : \n",
        "      j = clean(j)\n",
        "      just_anserws += j.split(\"Assistant:\")[1] + \" \"\n",
        "data = just_anserws\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wny8kKcW_a0H"
      },
      "source": [
        "# ***Making the data easier for the model :***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCAC5psL_1EH"
      },
      "outputs": [],
      "source": [
        "# create a tokenizer and fit it on training data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# convert texts to sequences\n",
        "encoded_text = tokenizer.texts_to_sequences([data])[0]\n",
        "\n",
        "Hi my name is iliya\n",
        "\n",
        "# normalize the data\n",
        "tf.keras.utils.normalize(encoded_text)\n",
        "\n",
        "# define the length of input sequences\n",
        "seq_length = 10\n",
        "\n",
        "input_sequences = []\n",
        "output_sequences = []\n",
        "for i in range(seq_length, len(encoded_text)):\n",
        "    input_sequences.append(encoded_text[i-seq_length:i])\n",
        "    output_sequences.append(encoded_text[i])\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFmVo5WcBHr2"
      },
      "source": [
        "# ***enable tracking experiment metrics :***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tHiB-ayBPW-"
      },
      "outputs": [],
      "source": [
        "# enable tracking experiment metrics\n",
        "\n",
        "tb_callback = TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True, write_images=True)\n",
        "earlystop_callback = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='auto')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPSvZ2R0Bau3"
      },
      "source": [
        "# ***make nural network model :***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-DbMHJ8B_vx"
      },
      "outputs": [],
      "source": [
        "# define the size of vocabulary\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# define the model for training on data\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size , 10 , input_length = seq_length))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(250, return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(250))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(vocab_size , activation = \"softmax\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKQcN6tuEC1B"
      },
      "outputs": [],
      "source": [
        "# compile the model for checking the format errors\n",
        "model.compile(loss='sparse_categorical_crossentropy',metrics = [\"accuracy\"], optimizer='adam')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLhgFAwBDWM5"
      },
      "source": [
        "# ***Fit the data on model and start training :***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MzBYGejrrJnu"
      },
      "outputs": [],
      "source": [
        "model.fit(input_sequences , output_sequences , epochs = 100 ,callbacks=[tb_callback, earlystop_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfvHd1QSDqQH"
      },
      "outputs": [],
      "source": [
        "# show tensorboard window\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkZzUUitEORY"
      },
      "source": [
        "# ***Predict and generate seed text :***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQbNczaZEpvh"
      },
      "outputs": [],
      "source": [
        "def gnr_txt(model , tok ,len_seq ,seed_text , num_tokns) :\n",
        "\n",
        "   # convert seed text to sequences \n",
        "   encoded_seed = tok.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "   # check encoded text \n",
        "   if len(encoded_seed) > len_seq : \n",
        "     encoded_seed = encoded_seed[-len_seq:]\n",
        "   else : \n",
        "     encoded_seed = [0] * (len_seq -len(encoded_seed) ) + encoded_seed\n",
        "\n",
        "   # predict the next word\n",
        "   for _ in range(num_tokns):\n",
        "        seed_sequence = np.reshape(encoded_seed, (1, seq_length))\n",
        "        prediction = model.predict(seed_sequence , use_multiprocessing=True)\n",
        "        next_token = np.argmax(prediction)\n",
        "        encoded_seed.append(next_token)\n",
        "        encoded_seed = encoded_seed[1:]\n",
        "\n",
        "    # convert sequences to texts\n",
        "   generated_text = tokenizer.sequences_to_texts([encoded_seed])[0]\n",
        "    \n",
        "   return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku70p9VHHnsV"
      },
      "outputs": [],
      "source": [
        "# The next word of the text we want to be specified\n",
        "seed_txt = \"i like to know who \"\n",
        "\n",
        "num_tkn = 10\n",
        "final_res = gnr_txt(model , tokenizer , seq_length , seed_txt ,num_tkn)\n",
        "print(seed_txt + final_res)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1lsL3ipAK6HnjOxWLc9d4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}